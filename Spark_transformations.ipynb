{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34435c60",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "Transformations are lazy operations on a RDD that create one or many new RDDs.\n",
    "* Transformations are functions that take a RDD as the input and produce one or many RDDs as the output. They do not change the input RDD (since RDDs are immutable and hence cannot be modified), but always produce one or more new RDDs by applying the computations they represent.\n",
    "* By applying transformations you incrementally build a RDD lineage with all the parent RDDs of the final RDD(s).\n",
    "* Transformations are lazy, i.e. are not executed immediately. Only after calling an action are transformations executed.\n",
    "\n",
    "###  Transformations Types\n",
    "\n",
    "Spark carefully distinguish \"transformation\" operation in two types:\n",
    "![Narrow vs wide transformations Image](assets/narrow_vs_wide_transformations.png)\n",
    "\n",
    "* **Narrow Transformations ** are the result of map, filter and such that is from the data from a single partition only, i.e. it is self-sustained.\n",
    "    * An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result.\n",
    "    * Spark groups narrow transformations as a stage which is called pipelining.\n",
    "    \n",
    "\n",
    "* **Wide Transformations ** are the result of groupByKey and reduceByKey. The data required to compute the records in a single partition may reside in many partitions of the parent RDD.\n",
    "    * All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute RDD shuffle, which transfers data across cluster and results in a new stage with a new set of partitions.\n",
    "\n",
    "\n",
    "###  Transformations dependecies\n",
    "\n",
    "The figure below gives a quick overview of the flow of a spark job\n",
    "![spark schedule process Image](assets/spark_schedule-process.png)\n",
    "\n",
    "\n",
    "One of the challenges in providing RDDs as an abstraction is choosing a representation for them that can \n",
    "track lineage across a wide range of transformations. The most interesting question in designing this  interface is how to represent dependencies between RDDs.\n",
    "\n",
    "It is both sufficient and useful to classify  dependencies into two types: \n",
    "* **Narrow dependencies**, where each partition of the parent RDD is used by at most one \n",
    "partition  of the child RDD.\n",
    "* **Wide dependencies**, where multiple child partitions may depend on it.\n",
    "\n",
    "![Transformation dependencies Image](assets/Transformation_Dependencies.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabdbe44",
   "metadata": {},
   "source": [
    "### General Transformations\n",
    "\n",
    "* **filter** (f)\n",
    "        \n",
    "    Return a new RDD containing only the elements that satisfy a predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39de42c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dce83b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(-5,5))          # Rango (-5, 5)\n",
    "filtered_rdd = rdd.filter(lambda x: x>=0)   # Devuelve los positivos\n",
    "print(filtered_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f1aa0e",
   "metadata": {},
   "source": [
    "* **map** (f, preservesPartitioning=False)\n",
    "    \n",
    "    Return a new RDD by applying a function to each element of this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11c1aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n"
     ]
    }
   ],
   "source": [
    "def add1(x):\n",
    "    return(x+1)\n",
    "\n",
    "squared_rdd = (filtered_rdd\n",
    "               .map(add1)\n",
    "               .map(lambda x: (x, x*x))) \n",
    "print(squared_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663bebe",
   "metadata": {},
   "source": [
    " * **flatMap** (f, preservesPartitioning=False)\n",
    "\n",
    "    Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "290e38ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 4, 3, 9, 4, 16, 5, 25]\n"
     ]
    }
   ],
   "source": [
    "squaredflat_rdd = (filtered_rdd\n",
    "                   .map(add1)\n",
    "                   .flatMap(lambda x: (x, x*x)))\n",
    "print(squaredflat_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d2bcb",
   "metadata": {},
   "source": [
    "* **distinct**(numPartitions=None)\n",
    "\n",
    "    Return a new RDD containing the distinct elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c01843b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 16, 1, 3, 9, 5, 25]\n"
     ]
    }
   ],
   "source": [
    "distinct_rdd = squaredflat_rdd.distinct()\n",
    "print(distinct_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4045aeb",
   "metadata": {},
   "source": [
    "* **groupBy**(f, numPartitions=None, partitionFunc=<function portable_hash at 0x7fc35dbc8e60>)\n",
    "\n",
    "    Return an RDD of grouped items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6d6d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, <pyspark.resultiterable.ResultIterable object at 0x7f8f50609c40>), (0, <pyspark.resultiterable.ResultIterable object at 0x7f8f505ac130>), (1, <pyspark.resultiterable.ResultIterable object at 0x7f8f505ac070>)]\n",
      "[(2, [2, 5]), (0, [3, 9]), (1, [1, 4, 16, 25])]\n"
     ]
    }
   ],
   "source": [
    "# Group values depending on the remainder of the division by 3\n",
    "grouped_rdd = distinct_rdd.groupBy(lambda x: x%3)\n",
    "print(grouped_rdd.collect())\n",
    "print([(x,sorted(y)) for (x,y) in grouped_rdd.collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b4b8a",
   "metadata": {},
   "source": [
    "### Math/Statistical Transformations\n",
    "\n",
    "   \n",
    "* **sample** (withReplacement, fraction, seed=None)\n",
    "\n",
    "    Return a sampled subset of this RDD.\n",
    "\n",
    "    Parameters:\t\n",
    "        * withReplacement – can elements be sampled multiple times (replaced when sampled out)\n",
    "        * fraction – expected size of the sample as a fraction of this RDD’s size without replacement: probability that each element is chosen; fraction must be [0, 1] with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
    "        * seed – seed for the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4e1cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1=[1, 3, 4, 25]\n",
      "s2=[1, 1, 2, 2, 4, 3, 3, 4, 4, 4, 16, 16, 5, 25, 25, 25]\n",
      "s3=[1, 2, 4, 3, 9, 4, 16, 5]\n"
     ]
    }
   ],
   "source": [
    "s1 = squaredflat_rdd.sample(False, 0.5).collect()\n",
    "s2 = squaredflat_rdd.sample(True, 2).collect()\n",
    "s3 = squaredflat_rdd.sample(False, 0.8).collect()\n",
    "print('s1={0}\\ns2={1}\\ns3={2}'.format(s1, s2, s3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda0e5a",
   "metadata": {},
   "source": [
    "### Set Theory /Relationan Transformations (transformations on two RDDs)\n",
    "\n",
    "* **union**(other)\n",
    "\n",
    "    Return the union of this RDD and another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f98d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'c', 'd', 'e']\n"
     ]
    }
   ],
   "source": [
    "rdda = sc.parallelize(['a', 'b', 'c'])\n",
    "rddb = sc.parallelize(['c', 'd', 'e'])\n",
    "rddu = rdda.union(rddb)\n",
    "print(rddu.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea556174",
   "metadata": {},
   "source": [
    "* **intersection** (other)\n",
    "\n",
    "    Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.\n",
    "    * This method performs a shuffle internally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "028e7172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c']\n"
     ]
    }
   ],
   "source": [
    "rddi = rdda.intersection(rddb)\n",
    "print(rddi.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee9fc53",
   "metadata": {},
   "source": [
    "* **subtract** (other, numPartitions=None)\n",
    "\n",
    "    Return each value in self that is not contained in other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a960ada8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'a']\n"
     ]
    }
   ],
   "source": [
    "rdds = rdda.subtract(rddb)\n",
    "print(rdds.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9807c4",
   "metadata": {},
   "source": [
    "* **zip** (other)\n",
    "\n",
    "    Zips this RDD with another one, returning key-value pairs with the first element in each RDD second element in each RDD, etc. \n",
    "    * Assumes that the two RDDs have the same number of partitions and the same number of elements in each partition (e.g. one was made through a map on the other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "687ccb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize(range(0,5))\n",
    "y = sc.parallelize(range(1000, 1005))\n",
    "x.zip(y).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
