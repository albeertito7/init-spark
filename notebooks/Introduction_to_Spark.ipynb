{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nuclear-holiday",
   "metadata": {},
   "source": [
    "# Introduction to Apache Spark\n",
    "\n",
    "## What is Spark?\n",
    "\n",
    "Spark Programming is nothing but a general-purpose & lightning fast cluster computing platform.\n",
    "\n",
    "- an Apache foundation **open source** project; not a product\n",
    "- **unified environment** for data scientists, developers and data engineers\n",
    "- enables highly **iterative** analysis on **massive** volumes of data at scale\n",
    "- an **in-memory computing** engine that works with **distributed** data, not a data store\n",
    "\n",
    "Designed to integrate with all the BigData tools. Like Spark can access any Hadoop data source.\\\n",
    "Extending Hadoop MapReduce to the next level.\n",
    "\n",
    "### Its relation with Hadoop\n",
    "\n",
    "Hadoop framework made a grasp on the market, as is based on a simple programming model (MapReduce introduced by Google) extensively used by companies enabling a scalable, flexible, fault-tolerant and cost effective computing solution.\n",
    "\n",
    "But Hadoop was designed for batch processing => no real-time data streaming\n",
    "\n",
    "<u>So, is Spark another competing technology to the famous framework?</u>\n",
    "\n",
    "Actually, **Spark is the natural evolution of Hadoop**, introduced for speeding up the Hadoop computational computing software process:\n",
    "\n",
    "- 100 times faster in-memory mode\n",
    "- 10 times faster running on disk\n",
    "\n",
    "allowing iterative queries and real-time Data Analytics using Spark Streaming.\n",
    "\n",
    "However, Spark is independent of Hadoop => it has its **own cluster management system**.\\\n",
    "Only uses Hadoop for storage purpose only.\n",
    "\n",
    "### Why choose Apache Spark?\n",
    "\n",
    "As we know, there was no general-purpose computing engine in the industry, since\n",
    "\n",
    "1. To perform batch processing, we were using Hadoop MapReduce.\n",
    "2. Also, to perform stream processing, we were using Apache Storm / S4.\n",
    "3. Moreover, for interactive processing, we were using Apache Impala / Apache Tez.\n",
    "4. To perform graph processing, we were using Neo4j / Apache Giraph.\n",
    "\n",
    "Hence, there was no powerful engine in the industry, that can process the data both in real-time and batch mode. Even, Big data is characterized by its velocity, volume, variety, value, and veracity due to which it needs to be processed at a higher speed.\n",
    "\n",
    "That's when Spark rises ★\n",
    "\n",
    "\n",
    "### Spark Features\n",
    "\n",
    "**1. Fast Processing:** saving time reducing the number of read-write operations to disk. \n",
    "\n",
    "**2. Flexibility:** supporting multiple languages, allows developers to write applications in **Java, Scala, R, or Python.**\n",
    "\n",
    "**3. In-memory processing:** stores data in the RAM => quick access => accelerates the speed of analytics.\n",
    "\n",
    "**4. Real-time processing:** designed to process real-time streaming data => instant outcomes.\n",
    "\n",
    "**5. Better analytics:** more than 80 high-level operators including:\n",
    "\n",
    "- not only MapReduce, but also\n",
    "- SQL queries\n",
    "- Streaming data\n",
    "- Machine learning algorithms\n",
    "- Graph algorithms\n",
    "    \n",
    "**6. Compatibility with Hadoop:** it can work on top of Hadoop as well, thus, it can read existing Hadoop data.\n",
    "\n",
    "**7. Easy to manage:** complete data analysis engine all integrated in the same cluster.\n",
    "\n",
    "- Hadoop provides only the batch-processing engine, needing different ones for each task.\n",
    "- With Spark there is no need for managing variuos Spark Components for each task.\n",
    "\n",
    "**8. Fault-tolerant:** Spark RDDs are designed to handle the failure of any worker node in the cluster ensuring the loss of data to zero.\n",
    "\n",
    "- designed to cover a wide range of workloads such as batch apps, iterative algorithms, interactive queries and streaming\n",
    "\n",
    "> Note: Resilient Distributed Datasets (RDD) is a fundamental immutable data structure of Spark. \n",
    "\n",
    "\n",
    "### Spark Motivation\n",
    "\n",
    "Current popular programming models for cluster transform data flowing from stable storage to stable storage.\n",
    "\n",
    "Example: MapReduce\n",
    "\n",
    "<div style=\"display: inline-block; text-align: left; margin: 15px 0px 15px 0px;\"><img src='../assets/mapReduceExample.jpg' alt='MapReduce example' width='580'/></div>\n",
    "\n",
    "**Benefits of data flow:** runtime can decide where to run tasks and can automatically recover from failures\n",
    "\n",
    "- Acyclic data flow is a powerful abstraction, but is not efficient for applications that repeatedly reuse a working set of data:\n",
    "    - **Iterative** algorithms (many in machine learning)\n",
    "    - **Interactive** data mining tools (R, Excel, Python)\n",
    "- Spark makes working sets a first-class concept to efficiently support these apps\n",
    "\n",
    "### Spark Goal\n",
    "\n",
    "- provide distributed memory abstractions for cluster to support apps with working sets\n",
    "- retain the attractive properties of MapReduce\n",
    "    1. fault tolerance (for crashes & stragglers)\n",
    "    2. data locality\n",
    "    3. scalability\n",
    "\n",
    "**Solution:** augment data flow model with \"resilient distributed datasets\" (RDDs)\n",
    "\n",
    "### Apache Spark is ...\n",
    "\n",
    "<div style=\"float:right; margin: 15px 0px 15px 0px;\"><img src='../assets/logistic-regression.png' alt='Logistic regression in Hadoop and Spark' width='250'/></div>\n",
    "\n",
    "#### 1. Fast\n",
    "- leverages aggressively cached in-memory distributed computing and JVM threads\n",
    "- faster than MapReduce (run workloads 100x faster)\n",
    "- apache spark achieves high performance for both batch and streaming data, using a state-of-the-art DAG scheduler, a query optimizer, and a physical execution engine\n",
    "\n",
    "\n",
    "#### 2. Ease of use (for programmers)\n",
    "- written in Scala, an object-oriented, functional programming language\n",
    "- Scala, Python, and Java APIs\n",
    "- Scala and Python interactive shells\n",
    "- runs on Hadoop, Mesos, Kubernetes standalone or cloud\n",
    "\n",
    "<div style=\"float:right; margin: 15px 0px 15px 0px;\"><img src='../assets/spark-stack.png' alt='Spart stack' width='250'/></div>\n",
    "\n",
    "#### 3. General purpose\n",
    "- covers a wide range of workloads\n",
    "- provides SQL, streaming and complex analytics\n",
    "- powers a stac of libraries including SQL and Dataframes, MLlib for machine learning, GraphX, and Spark Streaming\n",
    "- you can combine theses libraries seamlessly in the same application\n",
    "\n",
    "### Spark Stack\n",
    "\n",
    "- Spark Core Engine\n",
    "- Spark SQL\n",
    "- Spark Streaming\n",
    "- Mlib\n",
    "- GraphX\n",
    "\n",
    "Spark Core is the base upon the others are built/run.\n",
    "\n",
    "Spark SQL es el modulo para trabajar con datos estructurados.\n",
    "\n",
    "Spark Streaming es el componente que permite a Spark procesar datos en tiempo real.\n",
    "\n",
    "MLlib, hace que Spark esté equipado con una biblioteca de aprendizaje automatico. Contiene una amplia gama de algoritmos.\n",
    "\n",
    "GraphX, biblioteca que usa Spark para trabajar con grafos y hacer calculos sobre ellos.\n",
    "\n",
    "<div style=\"display: inline-block; text-aling: left; margin: 15px 0px 15px 0px;\"><img src='../assets/spark-complete-stack.png' alt='Spark complete stack' width='580'/></div>\n",
    "\n",
    "\n",
    "- procesamiento de datos en tiempo real\n",
    "    - gracias a spark streaming se puede procesar a tiempo real\n",
    "    - a diferencia de Hadoop MapReduce que sólo procesa los datos almacenados en HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assured-gardening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdf\n"
     ]
    }
   ],
   "source": [
    "print(\"asdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3efeaf8f12deeee38dec9f04e0b4bdf2695ae390c5ccbfd78431e919e9e4d08d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
