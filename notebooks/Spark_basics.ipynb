{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1182e7c",
   "metadata": {},
   "source": [
    "### First Steps\n",
    "1.- Create de Spark context (in local machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42edb2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=Spark-basics>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nA Spark application consists of a Driver Program and a group of Executors on the cluster.\\nThe Driver is a process that executes the amin program of your Spark application and creates the SparkContext.\\nThe SparkContext coordinates the execution of jobs.\\nThe executors are processes running on the worker nodes of the lcuster which are responsible for executing the tasks.\\nThe cluster manager (such as Mesos or YARN) is responsible for the allocation of physical resources to Spark Applications\\n\\n\\nEvery Spark application needs an entry point that allows it to communicate with data sources and perform certain operations such as reading and writing data.\\nIn Spark 1.x, three entry points were introduced:\\n    - SparkContext, SQLContext and HiveContext\\nSince Spark 2.x, a new entry point called SparkSession has been introduced that essentially combine all functionalities in the three aforementioned contexts.\\n\\n\\nSparkContext\\n\\n- used by the Driver Process in order to establish communication with the cluster and the resource managers to coordingate and execute jobs.\\n- SparkContext also enables the access to the other two contexts, namely SQLContext and HiveContext\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "\"\"\"\n",
    "Main entry point for Spark functionality, represents the connexion to the Spark cluster\n",
    "Can be used to cread RDD, accumulators and broadcast variables to that cluster\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "config = pyspark.SparkConf().setAppName('Spark-basics').setMaster('local[*]')\n",
    "sc = pyspark.SparkContext(conf=config)\n",
    "\n",
    "# you may prefer to create a Spark Configuration or not\n",
    "\"\"\"\n",
    "sc = pyspark.SparkContext(master='local[*]', appName='Spark-basics')\n",
    "\n",
    "\"\"\"\n",
    "master: Cluster URL to connect\n",
    "appName: Name for the job\n",
    "\"\"\"\n",
    "print(sc)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A Spark application consists of a Driver Program and a group of Executors on the cluster.\n",
    "The Driver is a process that executes the amin program of your Spark application and creates the SparkContext.\n",
    "The SparkContext coordinates the execution of jobs.\n",
    "The executors are processes running on the worker nodes of the lcuster which are responsible for executing the tasks.\n",
    "The cluster manager (such as Mesos or YARN) is responsible for the allocation of physical resources to Spark Applications\n",
    "\n",
    "Every Spark application needs an entry point that allows it to communicate with data sources and perform certain operations such as reading and writing data.\n",
    "In Spark 1.x, three entry points were introduced:\n",
    "    - SparkContext, SQLContext and HiveContext\n",
    "Since Spark 2.x, a new entry point called SparkSession has been introduced that essentially combine all functionalities in the three aforementioned contexts.\n",
    "\n",
    "\n",
    "SparkContext\n",
    "\n",
    "- used by the Driver Process in order to establish communication with the cluster and the resource managers to coordingate and execute jobs.\n",
    "- SparkContext also enables the access to the other two contexts, namely SQLContext and HiveContext\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "SQLContext is the entry point to SparkSQL which is a Spark module for structured data processing.\n",
    "Then the user can use it in order to perform various \"sql-like\" operations over Datasets and Dataframes.\n",
    "In order to create a SQLContext, you first must instantiate a SparkContext.\n",
    "\n",
    "# sql is a module of pyspark\n",
    "from pyspark.sql\n",
    "\n",
    "sc = pyspark.SparkContext(master='local[*]', appName='SQLContext')\n",
    "sql_context = pyspark.sql.SQLContext(sc)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "SparkSession\n",
    "- introduced in Spark 2.0\n",
    "- it is a new entry point that replace both SQLContext and HiveContext\n",
    "- giving immediate access to SparkContext\n",
    "\n",
    "#to join SparkContext with SQLContext (abstraction)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = pyspark.sql.SparkSession.builder.appName('SparkSession').getOrCreate()\n",
    "\n",
    "# even you can enable HiveSupport by\n",
    "spark_session = pyspark.sql.SparkSession.builder.appName('SparkSession-Hive-enabled').enableHiveSupport().getOrCreate()\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "if you are using Spark 2.x+, then you shouldn't really worry about HiveContext, SparkContext and SQLContext.\n",
    "All you have to do is to creat a SparkSession that offers support to Hive and sql-like operations.\n",
    "\n",
    "Additionally, in case you need to access SparkContext for any reason, you can still do it through SparkSession.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fdc4e5",
   "metadata": {},
   "source": [
    "2.- Read a input file, creating a RDD. Show the first record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d88763e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets/ml-100k/u.user MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1|24|M|technician|85711'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading users file\n",
    "user_data = sc.textFile(\"Datasets/ml-100k/u.user\") \n",
    "print(user_data)\n",
    "user_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581394aa",
   "metadata": {},
   "source": [
    "3.- Print some information about users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c06358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User record data: ['1', '24', 'M', 'technician', '85711']\n",
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "Some Statistics -> Users: 943, genders: 2, occupations: 21, ZIP codes: 795\n"
     ]
    }
   ],
   "source": [
    "# Get user data fields (string split separate by |)\n",
    "user_fields = user_data.map(lambda line: line.split(\"|\"))\n",
    "print(\"User record data: \" +format(user_fields.first()))\n",
    "# Get first field of all user records. Then print only the 10st (take(10)).\n",
    "print(user_fields.map(lambda fields: fields[0]).take(10))\n",
    "# Count the number of users (records).\n",
    "num_users = user_fields.map(lambda fields: fields[0]).count()\n",
    "# Count the number of distinct genders (fields[2])\n",
    "num_genders = user_fields.map(lambda fields:fields[2]).distinct().count()\n",
    "# Count the number of diferent ocupations (fields[3])\n",
    "num_occupations = user_fields.map(lambda fields:fields[3]).distinct().count()\n",
    "# Count the number of diferent zip codes (fields[4])\n",
    "num_zipcodes = user_fields.map(lambda fields:fields[4]).distinct().count()\n",
    "print (\"Some Statistics -> Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % \n",
    "(num_users, num_genders, num_occupations, num_zipcodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ab0ad",
   "metadata": {},
   "source": [
    "4. We can create a histogram to analyze the distribution of user ages, using matplotlib's hist function:\n",
    "    - We obtain a array of user ages.\n",
    "    - We passed in the ages array, together with the number of bins for our histogram (20 in this case), to the hist function. Using the normed=True argument, we also specified that we want the histogram to be normalized so that each bucket represents the percentage of the overall data that falls into that bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080c0f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 53, 23, 24, 33, 42, 57, 36, 29, 53]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1872x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the list of ages.\n",
    "ages = user_fields.map(lambda x: int(x[1])).collect()\n",
    "print(user_fields.map(lambda x: int(x[1])).take(10))\n",
    "# Create and plot the histogram\n",
    "plt.hist(ages, bins=20, color='lightblue', density=True)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(26, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682dc4a",
   "metadata": {},
   "source": [
    "### HELP Command\n",
    "\n",
    "You can use the `help` comand to get information about a spark/python function\n",
    "\n",
    "For example, you can get information about the map function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ed9a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class map in module builtins:\n",
      "\n",
      "class map(object)\n",
      " |  map(func, *iterables) --> map object\n",
      " |  \n",
      " |  Make an iterator that computes the function using arguments from\n",
      " |  each of the iterables.  Stops when the shortest iterable is exhausted.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __next__(self, /)\n",
      " |      Implement next(self).\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Return state information for pickling.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f898dd",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "RDDs represent a collection of items distributed across many compute nodes that can be manipulated in parallel.\n",
    "* Spark’s basic unit of data\n",
    "* Immutable, fault tolerant collection of records that can be distributed and  operated on in parallel across a cluster\n",
    "* Fault tolerance:\n",
    "    If data in memory is lost it will be recreated from lineage\n",
    "* Caching, persistence (memory, spilling, disk) and check-pointing\n",
    "* Many database or file type can be supported\n",
    "\n",
    "An RDD is physically distributed across the cluster, but manipulated as one logical entity:\n",
    "* Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster.\n",
    "* Spark will “distribute” any required processing to all partitions where the RDD exists and perform necessary redistributions and aggregations as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c925a",
   "metadata": {},
   "source": [
    "There are two ways to create RDDs: \n",
    "\n",
    "* Parallelizing an existing collection in your driver program.\n",
    "* Referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60381e4c",
   "metadata": {},
   "source": [
    "#### Parallelizing an existing collection\n",
    "\n",
    "Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbdbf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[23] at readRDDFromFile at PythonRDD.scala:274\n",
      "Numbers: ['uno', 'dos', 'tres']\n",
      "Array:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] .. [990, 991, 992, 993, 994, 995, 996, 997, 998, 999]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "numbers = sc.parallelize([\"uno\", \"dos\", \"tres\"])\n",
    "print(numbers)\n",
    "print(\"Numbers: \" +format(numbers.collect()))\n",
    "# Parallelize an array with 1000 elements\n",
    "array = sc.parallelize(np.array(range(1000)))\n",
    "print(\"Array:\" +format(array.take(10)) + \" .. \" + format(array.take(1000)[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d28d6ae",
   "metadata": {},
   "source": [
    "Once created, the distributed dataset can be operated on in parallel, depending on the number of partitions defined.\n",
    "* Spark will run one task for each partition of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac043c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499500\n"
     ]
    }
   ],
   "source": [
    "print(array.reduce(lambda a,b: a+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92714fee",
   "metadata": {},
   "source": [
    "Normally, Spark tries to set the number of partitions automatically based on your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870b95d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers partitions 2\n",
      "Array partitions 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Numbers partitions \" +format(array.getNumPartitions()))\n",
    "print(\"Array partitions \" +format(array.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c2ce55",
   "metadata": {},
   "source": [
    "However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10)).\n",
    "* Typically you want 2-4 partitions for each CPU in your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b8af014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Partitions: 10\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
      "Num Partitions (./Datasets/Books/*.txt): 4\n"
     ]
    }
   ],
   "source": [
    "# You can also set the number of partitions manually by passing it as a second parameter to parallelize\n",
    "data = range(100)\n",
    "distData = sc.parallelize(data,10)\n",
    "print(\"Num Partitions: \" + format(distData.getNumPartitions()))\n",
    "print(distData.glom().collect())\n",
    "text_file = sc.textFile(\"Datasets/Books/*.txt\")\n",
    "print(\"Num Partitions (./Datasets/Books/*.txt): \" + format(text_file.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ca34c3",
   "metadata": {},
   "source": [
    "#### Managing Spark Partitions with Coalesce and Repartition\n",
    "\n",
    "* **coalesce(numPartitions): **\tDecrease the number of partitions in the RDD to numPartitions. \n",
    "    Useful for running operations more efficiently after filtering down a large dataset. \n",
    "\n",
    "* ** repartition(numPartitions):**\tReshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. \n",
    "    This always shuffles all data over the network. \n",
    "    \n",
    "* ** partitionBy(numPartitions, partitionFunc): **  Partition by key, using a partitioning function (by default, a hash of the key)\n",
    "    * For key / value RDDs\n",
    "    * The same key to the same partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a413323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Num Partitions: 10\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
      "Increased Num Partitions: 20\n",
      "[[], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [], [], [], [], [], [], [], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [], [], [], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]]\n",
      "Decreased Num Partitions: 5\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]]\n",
      "Original Pairs Num Partitions: 5 -> [[('a', 2)], [('b', 5)], [('a', 8)], [('b', 6)], [('b', 1)]]\n",
      "Changed Pairs Num Partitions: 2 -> [[('b', 5), ('b', 6), ('b', 1)], [('a', 2), ('a', 8)]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Num Partitions: \" + format(distData.getNumPartitions()))\n",
    "print(distData.glom().collect())\n",
    "\n",
    "distData2 = distData.repartition(20)\n",
    "print(\"Increased Num Partitions: \" + format(distData2.getNumPartitions()))\n",
    "print(distData2.glom().collect())\n",
    "\n",
    "distData3 = distData2.coalesce(5)\n",
    "print(\"Decreased Num Partitions: \" + format(distData3.getNumPartitions()))\n",
    "print(distData3.glom().collect())\n",
    "\n",
    "pairs = sc.parallelize([(\"a\",2), (\"b\",5), (\"a\",8), (\"b\", 6), (\"b\",1)],5)\n",
    "print(\"Original Pairs Num Partitions: \" + format(pairs.getNumPartitions()) + \" -> \" +format(pairs.glom().collect()))\n",
    "sets = pairs.partitionBy(2)\n",
    "print(\"Changed Pairs Num Partitions: \" + format(sets.getNumPartitions()) + \" -> \" +format(sets.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f370c3f9",
   "metadata": {},
   "source": [
    "Keep in mind that repartitioning your data is a fairly expensive operation. the coalesce() Spark method is an optimized version of repartition() that allows avoiding data movement, but only if you are decreasing the number of RDD partitions. Coalece() avoids a full shuffle. If it's known that the number is decreasing then the executor can safely keep data on the minimum number of partitions, only moving the data off the extra nodes, onto the nodes that we kept.\n",
    "\n",
    "For example, with the following partitions: \n",
    "* Node 1 = 1,2,3 \n",
    "* Node 2 = 4,5,6 \n",
    "* Node 3 = 7,8,9 \n",
    "* Node 4 = 10,11,12\n",
    "\n",
    "Then coalesce down to 2 partitions: \n",
    "* Node 1 = 1,2,3 + (10,11,12) \n",
    "* Node 3 = 7,8,9 + (4,5,6)\n",
    "\n",
    "Notice that Node 1 and Node 3 did not require its original data to move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9fa6db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Partitions (./Datasets/Books/*.txt): 4\n",
      "Num Partitions (./Datasets/Books/*.txt): 2 completed in 0.045 seconds\n",
      "Num Partitions (./Datasets/Books/*.txt): 2 completed in 0.003 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test performance of repartition vs coalesce\n",
    "print(\"Num Partitions (./Datasets/Books/*.txt): \" + format(text_file.getNumPartitions()))\n",
    "from time import time\n",
    "t0 = time()\n",
    "text_file2 = text_file.repartition(2)\n",
    "tt = time() - t0\n",
    "print(\"Num Partitions (./Datasets/Books/*.txt): \" + format(text_file2.getNumPartitions()) + \" completed in {} seconds\".format(round(tt,3)))\n",
    "\n",
    "t0 = time()\n",
    "text_file3 = text_file.coalesce(2)\n",
    "tt = time() - t0\n",
    "print(\"Num Partitions (./Datasets/Books/*.txt): \" + format(text_file3.getNumPartitions()) + \" completed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfbeff3",
   "metadata": {},
   "source": [
    "##### Referencing an external dataset\n",
    "\n",
    "* **Reading and Writting text files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "735ced06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pg1661.txt.gz  file has 107533 words\n",
      "The pg2000.txt.gz  file has 384260 words\n",
      "The pg345.txt.gz   file has 164424 words\n",
      "The pg84.txt.gz    file has  77986 words\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "if os.path.exists(\"/home/jovyan/work/Results/WriteFiles\"): \n",
    "    shutil.rmtree(\"/home/jovyan/work/Results/WriteFiles\")\n",
    "if os.path.exists(\"Results/Text_File\"): \n",
    "    shutil.rmtree(\"Results/Text_File\")\n",
    "    \n",
    "# Reads all zip files in the directory and creates a partitioned list of lines\n",
    "lines = sc.textFile(\"Datasets/Books/Zipped/*.gz\")\n",
    "words = lines.flatMap(lambda x: x.split(\" \"))\n",
    "# Save the RDD words to several output files (one file per partition)\n",
    "words.saveAsTextFile(\"file:////home/jovyan/work/Results/WriteFiles\")\n",
    "        \n",
    "# Reads files and returns a key pair / key value -> filename, value-> complete file\n",
    "rdd=sc.wholeTextFiles(\"Datasets/Books/Zipped/\")\n",
    "\n",
    "# Get a key list / key value-> file name, value-> number of words\n",
    "flist = rdd.mapValues(lambda x: len(x.split()))\n",
    "for book in flist.collect():\n",
    "    print(\"The {0:14s} file has {1:6d} words\"\n",
    "          .format(book[0].split(\"/\")[-1], book[1]))\n",
    "    \n",
    "# Write File sizes.\n",
    "flist.saveAsTextFile(\"Results/Text_File\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a38f9d",
   "metadata": {},
   "source": [
    "* ** Reading and writing JSON files **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3487dc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twiter from AmeriGirl ->RT @NolteNC: Morning Joe gives huge credit to Hillary for confronting a critic but laughs hysterically when Cruz \"shows up\" to do same.\n",
      "{'id': 727497507379503108, 'name': 'Mica me sigue 13/8:)', 'text': 'RT @ArmyViciconte_: Se tiene que hacer YAAAAA #Con27EstasParaElBailandoMica', 'lang': 'es'}\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "if os.path.exists(\"Results/Tweets_ES.json\"): \n",
    "    shutil.rmtree(\"Results/Tweets_ES.json\")\n",
    "\n",
    "input = sc.textFile(\"Datasets/Tweets/tweets2.json\")\n",
    "import json\n",
    "data = input.map(lambda x: json.loads(x))\n",
    "print(\"Twiter from \" + format(data.first()['user']['name']) + \" ->\" +format(data.first()['text']))\n",
    "\n",
    "# Filter Spanish Tweets\n",
    "short_data = data.filter(lambda t: \"es\" in t[\"lang\"])\\\n",
    "                 .map(lambda t: {\"id\":t[\"id\"], \"name\":t[\"user\"][\"name\"], \"text\":t[\"text\"], \"lang\":t[\"lang\"]})\n",
    "print(short_data.first())\n",
    "# Write Spanish Tweets in a text/json file.\n",
    "short_data.saveAsTextFile(\"Results/Tweets_ES.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803fb900",
   "metadata": {},
   "source": [
    "* ** Read and Writting Sequence files **\n",
    "\n",
    "    Key/Value files used in Hadoop.\n",
    "    * Its elements implement the Writable interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68794296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 2)\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "if os.path.exists(\"Results/Sequence_files\"): \n",
    "    shutil.rmtree(\"Results/Sequence_files\")\n",
    "\n",
    "\n",
    "data = sc.parallelize([(\"a\",2), (\"b\",5), (\"a\",8)])\n",
    "# Save the RDD as sequence file.\n",
    "data.saveAsSequenceFile(\"Results/Sequence_files\")\n",
    "# Read sequence file\n",
    "seq = sc.sequenceFile(\"Results/Sequence_files\", \n",
    "                      \"org.apache.hadoop.io.Text\", \n",
    "                      \"org.apache.hadoop.io.IntWritable\")\n",
    "print(seq.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d43762",
   "metadata": {},
   "source": [
    "* ** Hadoop Input/Output Formats **\n",
    "\n",
    "    Spark can interact with any file format supported by Hadoop\n",
    "    * Supports \"old\" and \"new\" APIs\n",
    "    * It allows access to other types of storage (not file), e.g. HBase or MongoDB, via `saveAsHadoopDataSet` and/or `saveAsNewAPIHadoopDataSet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f76787fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', '2'), ('b', '5'), ('a', '8')]\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "if os.path.exists(\"Results/Hadoop_formats\"): \n",
    "    shutil.rmtree(\"Results/Hadoop_formats\")\n",
    "\n",
    "# Save the RDD as a Hadoop (TextOutputFormat)\n",
    "data.saveAsNewAPIHadoopFile(\"Results/Hadoop_formats\", \n",
    "                            \"org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\",\n",
    "                            \"org.apache.hadoop.io.Text\",\n",
    "                            \"org.apache.hadoop.io.IntWritable\")\n",
    "\n",
    "# Read it as a key-value file Hadoop (KeyValueTextInputFormat)\n",
    "rdd = sc.newAPIHadoopFile(\"Results/Hadoop_formats\", \n",
    "                          \"org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat\",\n",
    "                          \"org.apache.hadoop.io.Text\",\n",
    "                          \"org.apache.hadoop.io.IntWritable\")\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92bef58",
   "metadata": {},
   "source": [
    "* ** Object files **\n",
    "\n",
    "    Binary files that store any type of RDDs (not just key / value)\n",
    "    * In Python, pickle is used (`saveAsPickleFile()` and `pickleFile()` methods)\n",
    "    * Only useful to communicate Spark jobs with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019713a",
   "metadata": {},
   "source": [
    "### RDD Caching and Persistence\n",
    "\n",
    "Problem with using RDD multiple times:\n",
    "* Spark rebuilds the RDD and its dependencies every time an action is executed\n",
    "* Very expensive (especially in iterative problems)\n",
    "\n",
    "Solution\n",
    "* Keep the RDD in memory and / or disk\n",
    "* Methods cache () or persist ()\n",
    "\n",
    "Caching or persistence are optimisation techniques for (iterative and interactive) Spark computations. They help saving interim partial results so they can be reused in subsequent stages. These interim results as RDDs are thus kept in memory (default) or more solid storages like disk and/or replicated.\n",
    "\n",
    "RDDs can be cached using cache operation. They can also be persisted using persist operation.\n",
    "\n",
    "The difference between cache and persist operations is purely syntactic. cache is a synonym of persist or persist(MEMORY_ONLY), i.e. cache is merely persist with the default storage level MEMORY_ONLY.\n",
    "\n",
    "Sparks supports the following [levels of persistence][4], defined in pyspark.StorageLevel:\n",
    "![Spark persintence levels](assets/spark_persintence_levels.png)\n",
    "\n",
    "and the main characteristiques for each level are:\n",
    "\n",
    "![Spark persintence levels features](assets/Persitence_levels_features.png)\n",
    "\n",
    "\n",
    "#### Levels of persistence\n",
    "* In Python, the default level is MEMORY_ONLY_SER\n",
    "    * By default, Python serializes the data as pickled objects\n",
    "    * You can specify another serialization when creating the SparkContext\n",
    "        ```python\n",
    "        Sc = SparkContext (master = \"local\", appName = \"My app\", serializer = pyspark.MarshalSerializer ())\n",
    "        ```\n",
    "\n",
    "#### Troubleshooting\n",
    "* If a node with stored data fails, the RDD recovers\n",
    "    * By adding _2 to the persistence level, 2 copies of the RDD\n",
    "\n",
    "#### Managing the cache\n",
    "* LRU algorithm to manage the cache\n",
    "    * For memory-only levels, old RDDs are deleted and recalculated\n",
    "    * For memory and disk levels, partitions that do not fit are written to disk\n",
    "\n",
    "\n",
    "[4]: http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6f89819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. rdd is cached: False\n",
      "2. rdd is cached: True\n",
      "3. rdd2 is cached: False\n",
      "4. rdd2 is cached: True\n",
      "Persitence level for rdd: Disk Memory Serialized 2x Replicated \n",
      "Persitence level for rdd2: Memory Serialized 1x Replicated\n",
      "5. rdd is cached: False\n"
     ]
    }
   ],
   "source": [
    "#from test_helper import Test\n",
    "#from __future__ import print_function\n",
    "\n",
    "rdd = sc.parallelize(range(1000), 10)\n",
    "print(\"1. rdd is cached: \" +format(rdd.is_cached))\n",
    "\n",
    "rdd.persist(pyspark.StorageLevel.MEMORY_AND_DISK_2) ## MEMORY_AND_DISK_SER_2 deprecated use MEMORY_AND_DISK_2 instead\n",
    "print(\"2. rdd is cached: \" +format(rdd.is_cached))\n",
    "\n",
    "rdd2 = rdd.map(lambda x: x*x)\n",
    "print(\"3. rdd2 is cached: \" +format(rdd2.is_cached))\n",
    "\n",
    "\n",
    "rdd2.cache() # Default level\n",
    "print(\"4. rdd2 is cached: \" +format(rdd2.is_cached))\n",
    "\n",
    "print(\"Persitence level for rdd: {0} \".format(rdd.getStorageLevel()))\n",
    "print(\"Persitence level for rdd2: {0}\".format(rdd2.getStorageLevel()))\n",
    "\n",
    "rdd.unpersist() # keep out rdd from cache\n",
    "print(\"5. rdd is cached: \" +format(rdd.is_cached))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508c821",
   "metadata": {},
   "source": [
    "#### RDD Fault Tolerance (RDD log lineage)\n",
    "Spark operates on data in fault-tolerant file systems like HDFS or S3. So all the RDDs generated from fault tolerant data is fault tolerant. \n",
    "But this does not set true for streaming/live data (data over the network). So the key need of fault tolerance in Spark is for this kind of data. \n",
    "\n",
    "The basic fault-tolerant semantic of Spark are:\n",
    "* Since Apache Spark RDD is an immutable dataset, each Spark RDD remembers the lineage of the deterministic operation that was used on fault-tolerant input dataset to create it.\n",
    "* If due to a worker node failure any partition of an RDD is lost, then that partition can be re-computed from the original fault-tolerant dataset using the lineage of operations.\n",
    "* Assuming that all of the RDD transformations are deterministic, the data in the final transformed RDD will always be the same irrespective of failures in the Spark cluster.\n",
    "\n",
    "Each RDD maintains a pointer to one or more parent along with the metadata about what type of relationship it has with the parent. \n",
    "* For example, when we call val b = a.map() on a RDD, the RDD b just keeps a reference (and never copies) to its parent a, that's a **lineage**.\n",
    "\n",
    "When the driver submits the job, the RDD graph is serialized to the worker nodes so that each of the worker nodes apply the series of transformations (like, map filter and etc..) on different partitions. Also, this RDD lineage will be used to recompute the data if some failure occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a05105",
   "metadata": {},
   "source": [
    "To display the lineage of an RDD, Spark provides a debug method toDebugString() method.\n",
    "\n",
    "Consider the wordcount example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f943738b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words: 71478\n"
     ]
    }
   ],
   "source": [
    "text_file = sc.textFile(\"Datasets/Books/*.txt\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \"))\\\n",
    "                    .map(lambda word: (word, 1))\\\n",
    "                    .reduceByKey(lambda a, b: a + b)\n",
    "print(\"Number of Words: \" +format(counts.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a59f34",
   "metadata": {},
   "source": [
    "Executing toDebugString() on splitedLines RDD, will output the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad951a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(4) PythonRDD[100] at RDD at PythonRDD.scala:53 []\\n |  MapPartitionsRDD[98] at mapPartitions at PythonRDD.scala:145 []\\n |  ShuffledRDD[97] at partitionBy at NativeMethodAccessorImpl.java:0 []\\n +-(4) PairwiseRDD[96] at reduceByKey at <ipython-input-17-369a6da7456a>:2 []\\n    |  PythonRDD[95] at reduceByKey at <ipython-input-17-369a6da7456a>:2 []\\n    |  Datasets/Books/*.txt MapPartitionsRDD[94] at textFile at NativeMethodAccessorImpl.java:0 []\\n    |  Datasets/Books/*.txt HadoopRDD[93] at textFile at NativeMethodAccessorImpl.java:0 []'\n"
     ]
    }
   ],
   "source": [
    "print(counts.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a822eb",
   "metadata": {},
   "source": [
    "The two first lines (from bottom) shows the input RDD. We created this RDD by calling sc.textFile(). \n",
    "See below more diagrammatic view of the DAG graph created from a [similar RDD] [3].\n",
    "![DAG graph](assets/Lb3pQ.png)\n",
    "\n",
    "Once the DAG is build, Spark scheduler creates a physical execution plan. As mentioned above, the DAG scheduler splits the graph into multiple stages, the stages are created based on the transformations. The narrow transformations will be grouped (pipe-lined) together into a single stage. So for our example, Spark will create two stage execution as follows:\n",
    "![Stage Execution](assets/K4gJU.png)\n",
    "\n",
    "The DAG scheduler then submit the stages into the task scheduler. The number of tasks submitted depends on the number of partitions present in the textFile. Fox example consider we have 4 partitions in this example, then there will be 4 set of tasks created and submitted in parallel provided if there are enough slaves/cores. Below diagram illustrates this in bit more detail:\n",
    "![Tasks Execution](assets/GoYQB.png)\n",
    "\n",
    "[3]: https://stackoverflow.com/questions/30691385/internal-work-of-spark/30691654#30691654"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff340f",
   "metadata": {},
   "source": [
    "You can also set the number of partitions manually by passing it as a second parameter to parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e91ab49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Partitions: 10\n",
      "Num Partitions (./Datasets/Books/*): 8\n"
     ]
    }
   ],
   "source": [
    "# You can also set the number of partitions manually by passing it as a second parameter to parallelize\n",
    "data = range(100)\n",
    "distData = sc.parallelize(data,10)\n",
    "print(\"Num Partitions: \" + format(distData.getNumPartitions()))\n",
    "text_file = sc.textFile(\"Datasets/Books/*\")\n",
    "\n",
    "print(\"Num Partitions (./Datasets/Books/*): \" + format(text_file.getNumPartitions()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3efeaf8f12deeee38dec9f04e0b4bdf2695ae390c5ccbfd78431e919e9e4d08d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
