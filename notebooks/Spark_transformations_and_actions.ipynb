{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34435c60",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "Transformations are lazy operations on a RDD that create one or many new RDDs.\n",
    "* Transformations are functions that take a RDD as the input and produce one or many RDDs as the output. They do not change the input RDD (since RDDs are immutable and hence cannot be modified), but always produce one or more new RDDs by applying the computations they represent.\n",
    "* By applying transformations you incrementally build a RDD lineage with all the parent RDDs of the final RDD(s).\n",
    "* Transformations are lazy, i.e. are not executed immediately. Only after calling an action are transformations executed.\n",
    "\n",
    "###  Transformations Types\n",
    "\n",
    "Spark carefully distinguish \"transformation\" operation in two types:\n",
    "![Narrow vs wide transformations Image](assets/narrow_vs_wide_transformations.png)\n",
    "\n",
    "* **Narrow Transformations ** are the result of map, filter and such that is from the data from a single partition only, i.e. it is self-sustained.\n",
    "    * An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result.\n",
    "    * Spark groups narrow transformations as a stage which is called pipelining.\n",
    "    \n",
    "\n",
    "* **Wide Transformations ** are the result of groupByKey and reduceByKey. The data required to compute the records in a single partition may reside in many partitions of the parent RDD.\n",
    "    * All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute RDD shuffle, which transfers data across cluster and results in a new stage with a new set of partitions.\n",
    "\n",
    "\n",
    "###  Transformations dependecies\n",
    "\n",
    "The figure below gives a quick overview of the flow of a spark job\n",
    "![spark schedule process Image](assets/spark_schedule-process.png)\n",
    "\n",
    "\n",
    "One of the challenges in providing RDDs as an abstraction is choosing a representation for them that can \n",
    "track lineage across a wide range of transformations. The most interesting question in designing this  interface is how to represent dependencies between RDDs.\n",
    "\n",
    "It is both sufficient and useful to classify  dependencies into two types: \n",
    "* **Narrow dependencies**, where each partition of the parent RDD is used by at most one \n",
    "partition  of the child RDD.\n",
    "* **Wide dependencies**, where multiple child partitions may depend on it.\n",
    "\n",
    "![Transformation dependencies Image](assets/Transformation_Dependencies.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabdbe44",
   "metadata": {},
   "source": [
    "### General Transformations\n",
    "\n",
    "* **filter** (f)\n",
    "        \n",
    "    Return a new RDD containing only the elements that satisfy a predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39de42c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dce83b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(-5,5))          # Rang (-5, 5)\n",
    "filtered_rdd = rdd.filter(lambda x: x>=0)  # Returns the positive\n",
    "print(filtered_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f1aa0e",
   "metadata": {},
   "source": [
    "* **map** (f, preservesPartitioning=False)\n",
    "    \n",
    "    Return a new RDD by applying a function to each element of this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11c1aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n"
     ]
    }
   ],
   "source": [
    "def add1(x):\n",
    "    return(x+1)\n",
    "\n",
    "squared_rdd = (filtered_rdd\n",
    "               .map(add1)\n",
    "               .map(lambda x: (x, x*x))) \n",
    "print(squared_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663bebe",
   "metadata": {},
   "source": [
    " * **flatMap** (f, preservesPartitioning=False)\n",
    "\n",
    "    Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "290e38ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 4, 3, 9, 4, 16, 5, 25]\n"
     ]
    }
   ],
   "source": [
    "squaredflat_rdd = (filtered_rdd\n",
    "                   .map(add1)\n",
    "                   .flatMap(lambda x: (x, x*x)))\n",
    "print(squaredflat_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d2bcb",
   "metadata": {},
   "source": [
    "* **distinct**(numPartitions=None)\n",
    "\n",
    "    Return a new RDD containing the distinct elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c01843b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 16, 1, 3, 9, 5, 25]\n"
     ]
    }
   ],
   "source": [
    "distinct_rdd = squaredflat_rdd.distinct()\n",
    "print(distinct_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4045aeb",
   "metadata": {},
   "source": [
    "* **groupBy**(f, numPartitions=None, partitionFunc=<function portable_hash at 0x7fc35dbc8e60>)\n",
    "\n",
    "    Return an RDD of grouped items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6d6d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, <pyspark.resultiterable.ResultIterable object at 0x7f551bec2a00>), (0, <pyspark.resultiterable.ResultIterable object at 0x7f551befa190>), (1, <pyspark.resultiterable.ResultIterable object at 0x7f551befad30>)]\n",
      "[(2, [2, 5]), (0, [3, 9]), (1, [1, 4, 16, 25])]\n"
     ]
    }
   ],
   "source": [
    "# Group values depending on the remainder of the division by 3\n",
    "grouped_rdd = distinct_rdd.groupBy(lambda x: x%3)\n",
    "print(grouped_rdd.collect())\n",
    "print([(x,sorted(y)) for (x,y) in grouped_rdd.collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b4b8a",
   "metadata": {},
   "source": [
    "### Math/Statistical Transformations\n",
    "\n",
    "   \n",
    "* **sample** (withReplacement, fraction, seed=None)\n",
    "\n",
    "    Return a sampled subset of this RDD.\n",
    "\n",
    "    Parameters:\t\n",
    "        * withReplacement – can elements be sampled multiple times (replaced when sampled out)\n",
    "        * fraction – expected size of the sample as a fraction of this RDD’s size without replacement: probability that each element is chosen; fraction must be [0, 1] with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
    "        * seed – seed for the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4e1cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1=[4, 16, 5, 25]\n",
      "s2=[1, 1, 1, 1, 1, 1, 2, 4, 4, 3, 3, 9, 4, 16, 16, 16, 5, 25, 25]\n",
      "s3=[1, 1, 2, 4, 3, 9, 4, 16, 5]\n"
     ]
    }
   ],
   "source": [
    "s1 = squaredflat_rdd.sample(False, 0.5).collect()\n",
    "s2 = squaredflat_rdd.sample(True, 2).collect()\n",
    "s3 = squaredflat_rdd.sample(False, 0.8).collect()\n",
    "print('s1={0}\\ns2={1}\\ns3={2}'.format(s1, s2, s3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda0e5a",
   "metadata": {},
   "source": [
    "### Set Theory /Relationan Transformations (transformations on two RDDs)\n",
    "\n",
    "* **union**(other)\n",
    "\n",
    "    Return the union of this RDD and another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f98d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'c', 'd', 'e']\n"
     ]
    }
   ],
   "source": [
    "rdda = sc.parallelize(['a', 'b', 'c'])\n",
    "rddb = sc.parallelize(['c', 'd', 'e'])\n",
    "rddu = rdda.union(rddb)\n",
    "print(rddu.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea556174",
   "metadata": {},
   "source": [
    "* **intersection** (other)\n",
    "\n",
    "    Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.\n",
    "    * This method performs a shuffle internally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "028e7172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c']\n"
     ]
    }
   ],
   "source": [
    "rddi = rdda.intersection(rddb)\n",
    "print(rddi.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee9fc53",
   "metadata": {},
   "source": [
    "* **subtract** (other, numPartitions=None)\n",
    "\n",
    "    Return each value in self that is not contained in other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a960ada8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'a']\n"
     ]
    }
   ],
   "source": [
    "rdds = rdda.subtract(rddb)\n",
    "print(rdds.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9807c4",
   "metadata": {},
   "source": [
    "* **zip** (other)\n",
    "\n",
    "    Zips this RDD with another one, returning key-value pairs with the first element in each RDD second element in each RDD, etc. \n",
    "    * Assumes that the two RDDs have the same number of partitions and the same number of elements in each partition (e.g. one was made through a map on the other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "687ccb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize(range(0,5))\n",
    "y = sc.parallelize(range(1000, 1005))\n",
    "x.zip(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca329bad",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "Actions are RDD operations that produce non-RDD values. They materialize a value in a Spark program. In other words, a RDD operation that returns a value of any type but RDD[T] is an action.\n",
    "\n",
    "* The purpose of this action was to get data, from an RDD distributed across the nodes, back to our driver program. \n",
    "* If the results are too large to fit on the driver memory an important overhead can generated or even the driver can crash (there is an opportunity to write them directly to HDFS, instead). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d5f05",
   "metadata": {},
   "source": [
    "### Getting values actions\n",
    "\n",
    "\n",
    "* **collect**()\n",
    "\n",
    "    Return a list that contains all of the elements in this RDD.\n",
    "\n",
    "    *Note: This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86607909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'r', 'a', 'c', 'a', 'd', 'a', 'b', 'r', 'a']\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(list(\"abracadabra\")).cache()\n",
    "listrdd = rdd.collect()\n",
    "print(listrdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5b37e",
   "metadata": {},
   "source": [
    "* **take** (num)\n",
    "\n",
    "    Take the first num elements of the RDD.\n",
    "\n",
    "    It works by first scanning one partition, and use the results from that partition to estimate the number of additional partitions needed to satisfy the limit.\n",
    "\n",
    "\n",
    "\n",
    "* **sample** (withReplacement, fraction, seed=None)¶\n",
    "\n",
    "    Return a sampled subset of this RDD.\n",
    "    * withReplacement – can elements be sampled multiple times (replaced when sampled out)\n",
    "    * fraction – expected size of the sample as a fraction of this RDD’s size without replacement: probability that each element is chosen; fraction must be [0, 1] with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
    "    * seed – seed for the random number generator\n",
    "\n",
    "\n",
    "\n",
    "* **takeSample** (withReplacement, num, seed=None)\n",
    "\n",
    "    Return a fixed-size sampled subset of this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c020030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'r', 'a', 'c', 'a', 'd', 'a', 'b', 'r', 'a']\n",
      "['a', 'b', 'r', 'a']\n",
      "['a', 'b', 'a', 'c', 'a', 'b', 'a']\n",
      "['a', 'd', 'b', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(rdd.collect())\n",
    "t = rdd.take(4)\n",
    "print(t)\n",
    "s = rdd.sample(False, 0.5)\n",
    "print(s.collect())\n",
    "ts = rdd.takeSample(False, 4)\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f09f59e",
   "metadata": {},
   "source": [
    "* **top** (num, key=None)\n",
    "\n",
    "    Get the top N elements from an RDD.\n",
    "    \n",
    "    \n",
    "* **takeOrdered** (num, key=None)\n",
    "\n",
    "    Get the N elements from an RDD ordered in ascending order or as specified by the optional key function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d22c3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 7, 6]\n",
      "[1, 2, 3, 4]\n",
      "[10, 9, 7, 6]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).cache()\n",
    "t = rdd.top(4)\n",
    "print(t)\n",
    "o = rdd.takeOrdered(4)\n",
    "print(o)\n",
    "i = rdd.takeOrdered(4, lambda x: -x)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a3813",
   "metadata": {},
   "source": [
    "### Count Actions\n",
    "\n",
    "* **count**()\n",
    "\n",
    "    Return the number of elements in this RDD.\n",
    "    \n",
    "    \n",
    "* **countApprox** (timeout, confidence=0.95)\n",
    "\n",
    "    Approximate version of count() that returns a potentially incomplete result within a timeout, even if not all tasks have finished.\n",
    "    \n",
    "    \n",
    "    \n",
    "* **countApproxDistinct** (relativeSD=0.05)\n",
    "\n",
    "    Return approximate number of distinct elements in the RDD.\n",
    "    \n",
    "    * The algorithm used is based on streamlib’s implementation of “HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm”, available here.   \n",
    "    * relativeSD – Relative accuracy. Smaller values create counters that require more space. It must be greater than 0.000017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad6f9846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "20\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([i % 20 for i in range(1000)])\n",
    "print(rdd.count())\n",
    "print(rdd.distinct().count())\n",
    "print(900 < rdd.countApprox(1, 0.4) < 1100)\n",
    "print(15 < rdd.countApproxDistinct(0.5) < 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a0c03",
   "metadata": {},
   "source": [
    "* **countByValue**()\n",
    "\n",
    "    Return the count of each unique value in this RDD as a dictionary of (value, count) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28d6ad1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('a', 5), ('b', 2), ('r', 2), ('c', 1), ('d', 1)])\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(list(\"abracadabra\")).cache()\n",
    "dicc = rdd.countByValue()\n",
    "print(dicc.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bde87f",
   "metadata": {},
   "source": [
    "### Agregation Actions\n",
    "\n",
    "* **reduce** (f)\n",
    "\n",
    "    Reduces the elements of this RDD using the specified commutative and associative binary operator. Currently reduces partitions locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "014f11d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1,5))\n",
    "r = rdd.reduce(lambda x,y: x*y) # r = 1*2*3*4 = 24\n",
    "from operator import add\n",
    "s = rdd.reduce(add) # s = 1+2+3+4 = 10\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14467113",
   "metadata": {},
   "source": [
    "* **fold** (zeroValue, op)\n",
    "\n",
    "    Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral “zero value.”\n",
    "    Similar to `reduce`, but providing the identity value for the function (eg 0 for $+$; 1 for $\\times$, or `[]` for concatenation of lists)\n",
    "   \n",
    "    * The function op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object allocation; however, it should not modify t2.\n",
    "    * This behaves somewhat differently from fold operations implemented for non-distributed collections in functional languages like Scala. This fold operation may be applied to partitions individually, and then fold those results into the final result, rather than apply the fold to each element sequentially in some defined ordering. For functions that are not commutative, the result may differ from that of a fold applied to a non-distributed collection.\n",
    "\n",
    "   Similar to `reduce`, but providing the identity value for the function (eg 0 for $+$; 1 for $\\times$, or `[]` for concatenation of lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab6cf60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [-10, -9, -8, -7, -6, -5, -4], ['a', 'b', 'c']]\n",
      "[1, 2, 3, 4, -10, -9, -8, -7, -6, -5, -4, 'a', 'b', 'c']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([list(range(1,5)), list(range(-10,-3)), ['a', 'b', 'c']])\n",
    "print(rdd.collect())\n",
    "f = rdd.fold([], lambda x,y : x+y)\n",
    "print(f)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78227f1",
   "metadata": {},
   "source": [
    "* **aggregate** (zeroValue, seqOp, combOp)\n",
    "\n",
    "    Aggregate the elements of each partition, and then the results for all the partitions, using a given combine functions and a neutral “zero value.”\n",
    "    * The functions op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object allocation; however, it should not modify t2.\n",
    "    * The first function (seqOp) can return a different result type, U, than the type of this RDD. Thus, we need one operation for merging a T into an U and one operation for merging two U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f068211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 2, 3, 4, 5, 6, 7, 8], 40320.0, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "rdd = sc.parallelize(l, 2)\n",
    "seqOp  = (lambda acc, val: (acc[0]+[val], acc[1]*val, acc[2]+1))\n",
    "combOp = (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]*acc2[1], acc1[2]+acc2[2]))\n",
    "a = rdd.aggregate(([], 1., 0), seqOp, combOp) \n",
    "print(a)\n",
    "sc.parallelize([]).aggregate((0, 0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fa80f1",
   "metadata": {},
   "source": [
    "## Pair RDD Transformations\n",
    "\n",
    "They are RDDs that work with key pair / value (Pair RDDs)\n",
    "* Data types very used in Big Data (MapReduce)\n",
    "* Spark has special operations for handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37fbb0",
   "metadata": {},
   "source": [
    "Creating Pair RDDs\n",
    "\n",
    "    * From a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71862135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 5), ('a', 3)]\n",
      "[('a', 0), ('b', 1), ('c', 2)]\n"
     ]
    }
   ],
   "source": [
    "prdd = sc.parallelize([('a',2), ('b',5), ('a',3)])\n",
    "print(prdd.collect())\n",
    "prdd = sc.parallelize(zip(['a', 'b', 'c'], range(3)))\n",
    "print(prdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5f0d2",
   "metadata": {},
   "source": [
    "    * From another RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fc80e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair (1st word, line): ('burlaban', 'burlaban de Sancho; pero él se las tenía tiesas a todos, maguera tonto,')\n",
      "\n",
      "[(4, 2), (9, 3), (16, 4)]\n",
      "[('a', 0), ('b', 1), ('c', 2), ('d', 3), ('e', 4), ('f', 5), ('g', 6), ('h', 7)]\n",
      "RDD partition: [['a', 'b'], ['c', 'd'], ['e', 'f', 'g', 'h']]\n",
      "[('a', 0), ('b', 3), ('c', 1), ('d', 4), ('e', 2), ('f', 5), ('g', 8), ('h', 11)]\n"
     ]
    }
   ],
   "source": [
    "linesrdd = sc.textFile(\"./Datasets/Books/Quijote.txt\")\n",
    "prdd = linesrdd.map(lambda x : (x.split(\" \")[0], x))\n",
    "print(\"Pair (1st word, line): {0}\\n\".format(prdd.takeSample(False, 1)[0]))\n",
    "\n",
    "# keyBy(f): Create tuples of RDD elements using f to get the key.\n",
    "nrdd = sc.parallelize(range(2,5))\n",
    "prdd = nrdd.keyBy(lambda x: x*x)\n",
    "print(prdd.collect())\n",
    "\n",
    "# ZipWithIndex (): Zipe the RDD with the indexes of its elements.\n",
    "# This method triggers a spark job when the RDD has more than one partition.\n",
    "rdd = sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], 3)\n",
    "prdd = rdd.zipWithIndex()\n",
    "print(prdd.collect())\n",
    "\n",
    "# ZipWithUniqueId (): Zipe the RDD with unique identifiers (long) for each element.\n",
    "# The elements in the kth partition give the ids k, n + k, 2 * n + k, ... where n = number of partitions\n",
    "# Does not fire a spark job\n",
    "rdd = sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], 3)\n",
    "print(\"RDD partition: {0}\".format(rdd.glom().collect()))\n",
    "prdd = rdd.zipWithUniqueId()\n",
    "print(prdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d076eb",
   "metadata": {},
   "source": [
    "    * Using a zip of two RDDs\n",
    "      \n",
    "      RDDs must have the same number of partitions and the same number of elements in each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc62ee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(range(0,5), 2)\n",
    "rdd2 = sc.parallelize(range(1000, 1005), 2)\n",
    "prdd = rdd1.zip(rdd2)\n",
    "print(prdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d016e13c",
   "metadata": {},
   "source": [
    "### Transformations on a key / value RDD\n",
    "\n",
    "#### Aggregation Transformations\n",
    "\n",
    "* **reduceByKey** (func, numPartitions=None, partitionFunc=<function portable_hash at 0x7fc35dbc8e60>)¶\n",
    "\n",
    "    Merge the values for each key using an associative and commutative reduce function.\n",
    "\n",
    "    * This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a “combiner” in MapReduce.\n",
    "\n",
    "    * Output will be partitioned with numPartitions partitions, or the default parallelism level if numPartitions is not specified. Default partitioner is hash-partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8aa19db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 5), ('a', 10)]\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "prdd   = sc.parallelize([(\"a\", 2), (\"b\", 5), (\"a\", 8)]).cache()\n",
    "redrdd = prdd.reduceByKey(add)\n",
    "print(redrdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b568b",
   "metadata": {},
   "source": [
    "* **groupByKey** (numPartitions=None, partitionFunc=<function portable_hash at 0x7fc35dbc8e60>)\n",
    "\n",
    "    Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with numPartitions partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4f1881d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 1)]\n",
      "[('a', [2, 8]), ('b', [5])]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(prdd.groupByKey().mapValues(len).collect()))\n",
    "print(sorted(prdd.groupByKey().mapValues(list).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8336db63",
   "metadata": {},
   "source": [
    "* **combineByKey** (createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash at 0x7fc35dbc8e60>)\n",
    "\n",
    "    Generic function to combine the elements for each key using a custom set of aggregation functions.\n",
    "    * Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a “combined type” C.\n",
    "    * Users provide three functions:\n",
    "        * createCombiner, which turns a V into a C (e.g., creates a one-element list)\n",
    "        * mergeValue, to merge a V into a C (e.g., adds it to the end of a list)\n",
    "        * mergeCombiners, to combine two C’s into a single one.\n",
    "    * In addition, users can control the partitioning of the output RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1289b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', (12, 3)), ('a', (10, 2))]\n",
      "[('b', 4.0), ('a', 5.0)]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average per key using combineByKey ()\n",
    "l = sc.parallelize([(\"a\",2), (\"b\",5), (\"a\",8), (\"b\", 6), (\"b\",1)])\n",
    "\n",
    "# For each key, add and count the values\n",
    "sumCount = l.combineByKey(\n",
    "           (lambda x : (x, 1)),\n",
    "           (lambda x, y : (x[0]+y, x[1]+1)),\n",
    "           (lambda x, y : (x[0]+y[0], x[1]+y[1])))\n",
    "print(sumCount.collect())\n",
    "\n",
    "# Gets the mean of the values\n",
    "m = sumCount.mapValues(lambda v : v[0]/v[1])\n",
    "print(m.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f7a19",
   "metadata": {},
   "source": [
    "### Transformations on keys or values\n",
    "\n",
    "* **keys**() returns an RDD with the keys\n",
    "* **values**() returns an RDD with values\n",
    "* **sortByKey**() returns a key/value RDD with sorted keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1b96e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD complete: [('a', 2), ('b', 5), ('a', 8)]\n",
      "RDD with the keys: ['a', 'b', 'a']\n",
      "RDD with the values: [2, 5, 8]\n",
      "RDD with ordered keys: [('a', 2), ('a', 8), ('b', 5)]\n"
     ]
    }
   ],
   "source": [
    "print(\"RDD complete: {0}\".format(prdd.collect()))\n",
    "print(\"RDD with the keys: {0}\".format(prdd.keys().collect()))\n",
    "print(\"RDD with the values: {0}\".format(prdd.values().collect()))\n",
    "print(\"RDD with ordered keys: {0}\".format(prdd.sortByKey().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910ff8f",
   "metadata": {},
   "source": [
    "* **MapValues**(func):  returns an RDD by applying a function to the values\n",
    "* **FlatMapValues**(func): returns an RDD by applying a function on the values and \"flattening\" the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3323d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (2, 20)), ('b', (5, 50)), ('a', (8, 80))]\n",
      "[('a', 2), ('a', 20), ('b', 5), ('b', 50), ('a', 8), ('a', 80)]\n"
     ]
    }
   ],
   "source": [
    "mapv = prdd.mapValues(lambda x: (x, 10*x))\n",
    "print(mapv.collect())\n",
    "fmapv = prdd.flatMapValues(lambda x: (x, 10*x))\n",
    "print(fmapv.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c3a53",
   "metadata": {},
   "source": [
    "#### Transformations on two key / value RDDs\n",
    "\n",
    "* Join / rightOuterJoin / leftOuterJoin / fullOuterJoin performs inner / outer joins between the two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(\"a\",2), (\"b\",5), (\"a\",8)]).cache()\n",
    "rdd2 = sc.parallelize([(\"c\",7), (\"a\",1)]).cache()\n",
    "\n",
    "rdd3 = rdd1.join(rdd2)\n",
    "print(rdd3.collect())\n",
    "\n",
    "rdd4 = rdd1.leftOuterJoin(rdd2)\n",
    "print(rdd4.collect())\n",
    "\n",
    "rdd5 = rdd1.rightOuterJoin(rdd2)\n",
    "print(rdd5.collect())\n",
    "\n",
    "rdd6 = rdd1.fullOuterJoin(rdd2)\n",
    "print(rdd6.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382877b",
   "metadata": {},
   "source": [
    "* **SubtractByKey** removes elements with a key present in another RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3edeb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd7 = rdd1.subtractByKey(rdd2)\n",
    "print(rdd7.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994e4c4",
   "metadata": {},
   "source": [
    "* **Cogroup** groups data that shares the same key in both RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e7d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd8 = rdd1.cogroup(rdd2)\n",
    "print(rdd8.mapValues(lambda v: [list(l) for l in v]).collectAsMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd589c",
   "metadata": {},
   "source": [
    "### Actions on key/value RDDs\n",
    "\n",
    "* **CountByKey**() returns a map indicating the number of occurrences of each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3199fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prdd = sc.parallelize([(\"a\", 2), (\"b\", 5), (\"a\", 8)]).cache()\n",
    "countMap = prdd.countByKey()\n",
    "print(countMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a87132",
   "metadata": {},
   "source": [
    "* **CollectAsMap**() gets the RDD as a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddMap = prdd.collectAsMap()\n",
    "print(rddMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf81a30",
   "metadata": {},
   "source": [
    "* **Lookup**(key)  returns a list of values associated with a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9885ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "listA = prdd.lookup('a')\n",
    "print(listA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc248d",
   "metadata": {},
   "source": [
    "Numerical RDDs\n",
    "--------------\n",
    "\n",
    "Descriptive statistics functions implemented in Spark\n",
    "\n",
    "  | Method              | Description\n",
    "  | ------------------- | ----------------------------------\n",
    "  | Stats()             | Summary of statistics\n",
    "  | Mean()              | Arithmetic average\n",
    "  | Sum(), max(), min() | Sum, maximum and minimum\n",
    "  | Variance()          | Variance of the elements\n",
    "  | SampleVariance()    | Variance of a sample\n",
    "  | Stdev()             | Standard deviation\n",
    "  | SampleStdev()       | Standard deviation of a sample\n",
    "  | Histogram()         | Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdbadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab\n",
    "from math import fabs\n",
    "\n",
    "# An RDD with random data from a normal distribution\n",
    "nrdd = sc.parallelize(np.random.normal(size=10000)).cache()\n",
    "\n",
    "# Summary of statistics\n",
    "sts = nrdd.stats()\n",
    "print(\"Summary of statistics:\\n {0}\".format(sts))\n",
    "# Filters outliers\n",
    "stddev = sts.stdev()\n",
    "avg = sts.mean()\n",
    "frdd = nrdd.filter(lambda x: fabs(x - avg) < 3*stddev).cache()\n",
    "print(\"Number of outliers: {0}\".format(sts.count() - frdd.count()))\n",
    "\n",
    "# Gets a histogram with 10 groups\n",
    "x,y = frdd.histogram(10)\n",
    "plt.bar(x[:-1],y,width=0.6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
